{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraping news articles with TheGuardianAPI and BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook creates a labelled dataset for Natural Language Processing (NLP) by scraping news articles from different sections of the Guardian. \n",
    "\n",
    "First, it collects the location (urls) of the desired news articles using the Guardian Open Platform, specifically the content API endpoint. Then, it scrapes the text from each with the BeautifulSoup python library and saves it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, time\n",
    "import requests\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from pprint import pprint\n",
    "\n",
    "apikey = os.getenv('GUARDIAN_APIKEY')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can repeat this process to grab as many articles as needed. We will search for articles from 5 different sections of the newspaper, from Jan 1 2020. This type of query returns thousands of hits, over many pages. It is convenient to increase the `page-size` of the server response to the maximum value (200) and to iterate over the parameters `page` and `section`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_section_results(base_url, params, section, n_pages=3):\n",
    "    \n",
    "    section_results = []\n",
    "    current_page = 1\n",
    "    total_pages = n_pages\n",
    "    \n",
    "    while current_page <= total_pages:\n",
    "        params.update({'page': current_page, 'section': section})\n",
    "        try:\n",
    "            r = requests.get(base_url, params)\n",
    "            r.raise_for_status() \n",
    "        except requests.exceptions.RequestException as err:\n",
    "            raise SystemExit(err)\n",
    "            \n",
    "        data = r.json()\n",
    "        section_results.extend(data['response']['results'])\n",
    "        current_page += 1\n",
    "        \n",
    "    print(f\"Grabbed {len(section_results)} results.\")\n",
    "    return section_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def results_to_html(results, section, to_file=True):\n",
    "    \n",
    "    # grab urls, write to file\n",
    "    urls = [result['webUrl'] for result in results]\n",
    "    \n",
    "    if to_file:\n",
    "        with open(f\"urls/urls_{section}.txt\", 'w') as f:\n",
    "            for url in urls:\n",
    "                f.write(f\"{url}\\n\")\n",
    "        print(f\"Written urls to urls_{section}.txt.\")\n",
    "\n",
    "    # retrieve HTML from urls\n",
    "    html_files = {}\n",
    "    while len(html_files) < len(urls): \n",
    "        \n",
    "        for i, url in enumerate(urls):\n",
    "            if i not in html_files:\n",
    "                try:\n",
    "                    file = requests.get(url)\n",
    "                    file.raise_for_status()\n",
    "                    html_files[i] = file\n",
    "                except requests.exceptions.RequestException as err:\n",
    "                    with open(\"logs.txt\", 'a') as f:\n",
    "                        f.write(f\"At file {i}: {err}\\n\")\n",
    "                    time.sleep(10)\n",
    "    \n",
    "    # tests\n",
    "    codes = set(file.status_code for file in html_files.values())\n",
    "    \n",
    "    if len(html_files) == len(urls):\n",
    "        print(f\"Retrieved html responses for all {len(urls)} urls. Status codes remaining: {codes}.\")\n",
    "        return html_files\n",
    "    else:\n",
    "        sys.exit(f\"Error: only got {len(html_files)} articles for {len(urls)} urls. Codes: {codes}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def html_to_text(html_files, section):\n",
    "    \n",
    "    all_texts = []\n",
    "    \n",
    "    for file_id, file in html_files.items():\n",
    "        soup = BeautifulSoup(file.content, 'html.parser')\n",
    "        body = soup.find_all('div', class_='article-body-commercial-selector')\n",
    "        \n",
    "        # items with no body are (few) liveblogs, can ignore\n",
    "        if len(body) == 1:\n",
    "            if section in ['film', 'culture']:\n",
    "                ps = body[0].find_all('p', class_='dcr-1m34hpq')\n",
    "            else:\n",
    "                ps = body[0].find_all('p', class_='dcr-s23rjr')\n",
    "            par_list = [p.text for p in ps]\n",
    "            text = \" \".join(par_list)\n",
    "            \n",
    "            # replace the Unicode-converted HTML entities\n",
    "            text = text.replace('\\xa0', ' ')\n",
    "            \n",
    "            # discard items not encompassed by above tags\n",
    "            if not (text == ''):\n",
    "                all_texts.append(text)\n",
    "            else:\n",
    "                # keep a record of \"bad\" urls\n",
    "                with open('bad_p.txt', 'a') as f:\n",
    "                    f.write(f\"Section {section}: {file.url}\\n\")\n",
    "    \n",
    "    print(f\"All done! Correctly parsed documents: {len(all_texts)}, discarded: {len(html_files)-len(all_texts)}.\")\n",
    "    return all_texts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's grab the first 3 pages (600 items) of each section. After discarding some items due to inconsistent parsing we'll be left with a sizeable dataset with around 2500 news articles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    \n",
    "    API_ENDPOINT = \"http://content.guardianapis.com/search\"\n",
    "    sections = ['environment', 'business', 'film', 'culture', 'education']\n",
    "    my_params = {\n",
    "        'api-key': apikey,\n",
    "        'order-by': 'relevance', \n",
    "        'from-date': \"2020-1-1\",\n",
    "        'page-size': 200,\n",
    "    }\n",
    "    \n",
    "    for i, section in enumerate(sections):\n",
    "        \n",
    "        print(f\"[{i+1}/{len(sections)}] Requesting '{section}' articles from api...\")\n",
    "        results = get_section_results(API_ENDPOINT, my_params, section, n_pages=3)\n",
    "        \n",
    "        print(f\"Requesting html contents...\")\n",
    "        html_files = results_to_html(results, section, to_file=True)\n",
    "        \n",
    "        print(f\"Parsing and cleaning contents...\")\n",
    "        texts = html_to_text(html_files, section)\n",
    "        \n",
    "        print(f\"Saving dataframe...\")\n",
    "        df = pd.DataFrame({'Content': texts, 'Section': section})\n",
    "        df.to_csv(f\"data/{section}_news.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/5] Requesting 'environment' articles from api...\n",
      "Grabbed 600 results.\n",
      "Requesting html contents...\n",
      "Written urls to urls_environment.txt.\n",
      "Retrieved html responses for all 600 urls. Status codes remaining: {200}.\n",
      "Parsing and cleaning contents...\n",
      "All done! Correctly parsed documents: 585, discarded: 15.\n",
      "Saving dataframe...\n",
      "[2/5] Requesting 'business' articles from api...\n",
      "Grabbed 600 results.\n",
      "Requesting html contents...\n",
      "Written urls to urls_business.txt.\n",
      "Retrieved html responses for all 600 urls. Status codes remaining: {200}.\n",
      "Parsing and cleaning contents...\n",
      "All done! Correctly parsed documents: 475, discarded: 125.\n",
      "Saving dataframe...\n",
      "[3/5] Requesting 'film' articles from api...\n",
      "Grabbed 600 results.\n",
      "Requesting html contents...\n",
      "Written urls to urls_film.txt.\n",
      "Retrieved html responses for all 600 urls. Status codes remaining: {200}.\n",
      "Parsing and cleaning contents...\n",
      "All done! Correctly parsed documents: 599, discarded: 1.\n",
      "Saving dataframe...\n",
      "[4/5] Requesting 'culture' articles from api...\n",
      "Grabbed 600 results.\n",
      "Requesting html contents...\n",
      "Written urls to urls_culture.txt.\n",
      "Retrieved html responses for all 600 urls. Status codes remaining: {200}.\n",
      "Parsing and cleaning contents...\n",
      "All done! Correctly parsed documents: 572, discarded: 28.\n",
      "Saving dataframe...\n",
      "[5/5] Requesting 'education' articles from api...\n",
      "Grabbed 600 results.\n",
      "Requesting html contents...\n",
      "Written urls to urls_education.txt.\n",
      "Retrieved html responses for all 600 urls. Status codes remaining: {200}.\n",
      "Parsing and cleaning contents...\n",
      "All done! Correctly parsed documents: 523, discarded: 77.\n",
      "Saving dataframe...\n"
     ]
    }
   ],
   "source": [
    "!rm bad_p.txt\n",
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check saved data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "585\n",
      "475\n",
      "599\n",
      "572\n",
      "523\n"
     ]
    }
   ],
   "source": [
    "sections = ['environment', 'business', 'film', 'culture', 'education']\n",
    "\n",
    "for section in sections:\n",
    "    tmp = pd.read_csv(f\"data/{section}_news.csv\")\n",
    "    print(len(tmp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(475, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Content</th>\n",
       "      <th>Section</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A consortium that includes high street giant N...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Connells has struck an agreed offer for Countr...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>UK mortgage approvals have risen to the highes...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Another 787,000 Americans filed for unemployme...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Primark has said it will lose an additional £2...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Content   Section\n",
       "0  A consortium that includes high street giant N...  business\n",
       "1  Connells has struck an agreed offer for Countr...  business\n",
       "2  UK mortgage approvals have risen to the highes...  business\n",
       "3  Another 787,000 Americans filed for unemployme...  business\n",
       "4  Primark has said it will lose an additional £2...  business"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"data/business_news.csv\")\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we are done. Another way to save text data would be to save each article to a separate .txt file, but for now this will suffice."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
