{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1966bd82",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, time\n",
    "import requests\n",
    "import random\n",
    "import pickle\n",
    "import spacy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from pprint import pprint\n",
    "\n",
    "import dash\n",
    "import dash_table as dt\n",
    "import dash_core_components as dcc\n",
    "import dash_html_components as html\n",
    "from dash.dependencies import Input, Output, State\n",
    "import plotly.express as px\n",
    "\n",
    "from PIL import Image\n",
    "import glob\n",
    "\n",
    "apikey = os.getenv('GUARDIAN_APIKEY')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf8de8ce",
   "metadata": {},
   "source": [
    "## Scraping the inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0afe9d2",
   "metadata": {},
   "source": [
    "Re-purpose the code from dataset creation to scrape just 5 new articles, this time from any section of the Guardian."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c67f3bc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions\n",
    "def get_results(base_url, params):\n",
    "    \n",
    "    results = []\n",
    "    try:\n",
    "        r = requests.get(base_url, params)\n",
    "        r.raise_for_status()\n",
    "    except requests.exceptions.RequestException as err:\n",
    "        raise SystemExit(err)\n",
    "    \n",
    "    data = r.json()\n",
    "    results.extend(data['response']['results'])\n",
    "    return results\n",
    "\n",
    "\n",
    "def results_to_html(results):\n",
    "    \n",
    "    # grab urls, write to file\n",
    "    urls = [result['webUrl'] for result in results]\n",
    "\n",
    "    # retrieve HTML from urls\n",
    "    html_files = {}\n",
    "    while len(html_files) < len(urls): \n",
    "        \n",
    "        for i, url in enumerate(urls):\n",
    "            if i not in html_files:\n",
    "                try:\n",
    "                    file = requests.get(url)\n",
    "                    file.raise_for_status()\n",
    "                    html_files[i] = file\n",
    "                except requests.exceptions.RequestException as err:\n",
    "                    time.sleep(2)\n",
    "    \n",
    "    return html_files\n",
    "        \n",
    "        \n",
    "def html_to_text(html_files):\n",
    "    \n",
    "    all_texts = []\n",
    "    \n",
    "    for file_id, file in html_files.items():\n",
    "        soup = BeautifulSoup(file.content, 'html.parser')\n",
    "        body = soup.find_all('div', class_='article-body-commercial-selector')\n",
    "        if len(body) == 1:\n",
    "            ps = body[0].find_all('p')\n",
    "        par_list = [p.text for p in ps]\n",
    "        text = \" \".join(par_list)\n",
    "        text = text.replace('\\xa0', ' ')\n",
    "        if not (text == ''):\n",
    "            all_texts.append(text)\n",
    "        \n",
    "    return all_texts\n",
    "\n",
    "# Main function to scrape and collect news articles for inference\n",
    "def get_text():\n",
    "    \n",
    "    API_ENDPOINT = \"http://content.guardianapis.com/search\"\n",
    "    # pick a random page, to display different results\n",
    "    page_number = random.randint(0,15)\n",
    "    my_params = {\n",
    "        'api-key': apikey,\n",
    "        'order-by': 'relevance', \n",
    "        'from-date': \"2020-1-1\",\n",
    "        'page-size': 10,\n",
    "        'page': page_number\n",
    "    }\n",
    "    \n",
    "    results = get_results(API_ENDPOINT, my_params)\n",
    "    html_files = results_to_html(results)\n",
    "    texts = html_to_text(html_files)\n",
    "    df = pd.DataFrame({'Content': texts})\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3b7ace13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Jazz bassist Eugene Wright, who was the last s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>For France’s heliciculteurs, or snail farmers,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>In parts of England, parents face more uncerta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Far be it from the Rumour Mill to claim to kno...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Far be it from the Rumour Mill to claim to kno...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>New Zealand has further tightened border contr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Lawyers for a Georgia father and son accused o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>The only black composer has been dropped from ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>I lived with an alcoholic for a decade without...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>New South Wales has recorded two locally acqui...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Content\n",
       "0  Jazz bassist Eugene Wright, who was the last s...\n",
       "1  For France’s heliciculteurs, or snail farmers,...\n",
       "2  In parts of England, parents face more uncerta...\n",
       "3  Far be it from the Rumour Mill to claim to kno...\n",
       "4  Far be it from the Rumour Mill to claim to kno...\n",
       "5  New Zealand has further tightened border contr...\n",
       "6  Lawyers for a Georgia father and son accused o...\n",
       "7  The only black composer has been dropped from ...\n",
       "8  I lived with an alcoholic for a decade without...\n",
       "9  New South Wales has recorded two locally acqui..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = get_text()\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "becd3729",
   "metadata": {},
   "source": [
    "## From inputs to category prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed1e51c9",
   "metadata": {},
   "source": [
    "Load trained classifier and TF-IDF vectoriser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1780b8f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"models/best_svm.pickle\", 'rb') as model:\n",
    "    svc = pickle.load(model)\n",
    "    \n",
    "with open(\"processed/tfidf_vectoriser.pickle\", 'rb') as f:\n",
    "    vectoriser = pickle.load(f)\n",
    "    \n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a58ed2b",
   "metadata": {},
   "source": [
    "Create dictionary to convert predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ece946de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'environment', 1: 'business', 2: 'film', 3: 'culture', 4: 'education', 5: 'not sure'}\n"
     ]
    }
   ],
   "source": [
    "# create dictionary\n",
    "sections = ['environment', 'business', 'film', 'culture', 'education']\n",
    "d = {i: section for i, section in enumerate(sections)}\n",
    "\n",
    "# add the \"other\" category when model is unsure\n",
    "d.update({5: \"not sure\"})\n",
    "print(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8d718c0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions\n",
    "def keep_token(t):\n",
    "    \"\"\"Decide whether to keep a token\"\"\"\n",
    "    return (t.is_alpha and not (t.is_space or t.is_punct or t.is_stop))\n",
    "\n",
    "\n",
    "def lemmatised_string(doc):\n",
    "    \"\"\"Lemmatise remaining tokens\"\"\"\n",
    "    return \" \".join(t.lemma_ for t in doc if keep_token(t))\n",
    "\n",
    "\n",
    "def predict_category(features):\n",
    "    initial_preds = svc.predict(features)\n",
    "    probs = svc.predict_proba(features).max(axis=1)\n",
    "    preds = []\n",
    "    \n",
    "    for prob, in_pred in zip(probs, initial_preds):\n",
    "        if prob > 0.5:\n",
    "            preds.append(in_pred)\n",
    "        else:\n",
    "            preds.append(5)\n",
    "    return [d[pred] for pred in preds], probs\n",
    "\n",
    "# Main function \n",
    "def clean_text_and_predict(df_0):\n",
    "    \n",
    "    df = df_0.copy()\n",
    "    df['Content_parsed'] = df['Content'].str.lower()\n",
    "    df['Content_parsed'] = df['Content_parsed'].str.strip()\n",
    "    \n",
    "    # parse and clean articles\n",
    "    docs = list(nlp.pipe(df['Content_parsed'], disable=['tok2vec','ner','tagger','parser']))\n",
    "    df['Content_parsed'] = [lemmatised_string(doc) for doc in docs]\n",
    "    \n",
    "    # numericalise with learnt transformer\n",
    "    features = vectoriser.transform(df['Content_parsed']).toarray()\n",
    "    \n",
    "    preds, probs = predict_category(features)\n",
    "    df['Prediction'] = preds\n",
    "    df['Confidence'] = probs\n",
    "    df['Confidence'] = (df['Confidence']*100).map('{:,.0f}%'.format)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "29523ae6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Content</th>\n",
       "      <th>Content_parsed</th>\n",
       "      <th>Prediction</th>\n",
       "      <th>Confidence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Jazz bassist Eugene Wright, who was the last s...</td>\n",
       "      <td>jazz bassist eugene wright survive member dave...</td>\n",
       "      <td>culture</td>\n",
       "      <td>63%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>For France’s heliciculteurs, or snail farmers,...</td>\n",
       "      <td>france heliciculteurs snail farmer desperately...</td>\n",
       "      <td>business</td>\n",
       "      <td>81%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>In parts of England, parents face more uncerta...</td>\n",
       "      <td>part england parent face uncertainty local aut...</td>\n",
       "      <td>education</td>\n",
       "      <td>99%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Far be it from the Rumour Mill to claim to kno...</td>\n",
       "      <td>far rumour mill claim know go mind roman abram...</td>\n",
       "      <td>not sure</td>\n",
       "      <td>45%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Far be it from the Rumour Mill to claim to kno...</td>\n",
       "      <td>far rumour mill claim know go mind roman abram...</td>\n",
       "      <td>not sure</td>\n",
       "      <td>45%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>New Zealand has further tightened border contr...</td>\n",
       "      <td>new zealand tighten border control amid mount ...</td>\n",
       "      <td>not sure</td>\n",
       "      <td>45%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Lawyers for a Georgia father and son accused o...</td>\n",
       "      <td>lawyer georgia father son accuse pursue shoot ...</td>\n",
       "      <td>not sure</td>\n",
       "      <td>50%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>The only black composer has been dropped from ...</td>\n",
       "      <td>black composer drop syllabus popular level mus...</td>\n",
       "      <td>education</td>\n",
       "      <td>99%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>I lived with an alcoholic for a decade without...</td>\n",
       "      <td>live alcoholic decade realise hold office job ...</td>\n",
       "      <td>culture</td>\n",
       "      <td>63%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>New South Wales has recorded two locally acqui...</td>\n",
       "      <td>new south wale record locally acquire case lin...</td>\n",
       "      <td>not sure</td>\n",
       "      <td>45%</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Content  \\\n",
       "0  Jazz bassist Eugene Wright, who was the last s...   \n",
       "1  For France’s heliciculteurs, or snail farmers,...   \n",
       "2  In parts of England, parents face more uncerta...   \n",
       "3  Far be it from the Rumour Mill to claim to kno...   \n",
       "4  Far be it from the Rumour Mill to claim to kno...   \n",
       "5  New Zealand has further tightened border contr...   \n",
       "6  Lawyers for a Georgia father and son accused o...   \n",
       "7  The only black composer has been dropped from ...   \n",
       "8  I lived with an alcoholic for a decade without...   \n",
       "9  New South Wales has recorded two locally acqui...   \n",
       "\n",
       "                                      Content_parsed Prediction Confidence  \n",
       "0  jazz bassist eugene wright survive member dave...    culture        63%  \n",
       "1  france heliciculteurs snail farmer desperately...   business        81%  \n",
       "2  part england parent face uncertainty local aut...  education        99%  \n",
       "3  far rumour mill claim know go mind roman abram...   not sure        45%  \n",
       "4  far rumour mill claim know go mind roman abram...   not sure        45%  \n",
       "5  new zealand tighten border control amid mount ...   not sure        45%  \n",
       "6  lawyer georgia father son accuse pursue shoot ...   not sure        50%  \n",
       "7  black composer drop syllabus popular level mus...  education        99%  \n",
       "8  live alcoholic decade realise hold office job ...    culture        63%  \n",
       "9  new south wale record locally acquire case lin...   not sure        45%  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_text_and_predict(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4885b4b2",
   "metadata": {},
   "source": [
    "## Create app in Dash"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84e331c7",
   "metadata": {},
   "source": [
    "Create"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2192d633",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'business.jpeg'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glob.glob(f'/Users/michele/github/Guardian_news_classification/assets/business.*')[0].split('/')[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e45b332b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_list = []\n",
    "for filename in glob.glob(f'/Users/michele/github/Guardian_news_classification/images/*.png'):\n",
    "    im=Image.open(filename)\n",
    "    image_list.append(im)\n",
    "\n",
    "image_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "afa4388c",
   "metadata": {},
   "outputs": [],
   "source": [
    "external_stylesheets = ['https://codepen.io/chriddyp/pen/bWLwgP.css']\n",
    "\n",
    "app = dash.Dash(__name__, external_stylesheets=external_stylesheets)\n",
    "\n",
    "colors = {\n",
    "    'background': '#FFFFFF',#'#212F3D',\n",
    "    'text': '#000000' #'#FBFCFC'\n",
    "}\n",
    "\n",
    "#current_dir = os.getcwd() + \"/images/\"\n",
    "\n",
    "markdown = \"\"\"\n",
    "This app predicts the topic of different bodies of text, either input by the user or news articles scraped from the Guardian.\n",
    "\"\"\"\n",
    "\n",
    "app.layout = html.Div(style={'backgroundColor': colors['background']}, children=[   \n",
    "\n",
    "    # title\n",
    "    html.H1(\n",
    "        children='Wanna see if I can guess...',\n",
    "        style={\n",
    "            'textAlign': 'center',\n",
    "            'color': colors['text']\n",
    "        }\n",
    "    ),\n",
    "    \n",
    "    # space\n",
    "    html.Br(),\n",
    "    \n",
    "    # main section\n",
    "    html.Div([\n",
    "        \n",
    "        # LEFT PANEL (prediction on user input)\n",
    "        html.Div([\n",
    "            html.H3(\"...what you are writing about?\"),\n",
    "            \n",
    "            dcc.Textarea(id='input-state',\n",
    "                         placeholder='Write here',\n",
    "                         style={'width': 500, 'height': 120}),\n",
    "            \n",
    "            html.Div([\n",
    "                html.Button(id='submit-button-1', n_clicks=0, children='Let me guess',\n",
    "                        style={'marginBottom':50})\n",
    "            ]),\n",
    "                       \n",
    "            html.H5(id='output-1'),\n",
    "            \n",
    "            html.Br(),\n",
    "            \n",
    "            html.Div(id='image-1', style={'marginTop': 100})\n",
    "        ],\n",
    "        style={'width': '45%', 'display': 'inline-block', 'textAlign': 'center', 'verticalAlign': 'top'}),\n",
    "        \n",
    "        \n",
    "        # RIGHT PANEL (prediction on scraped news)\n",
    "        html.Div([\n",
    "            html.H3(\"...what the Guardian is writing about?\"),\n",
    "            \n",
    "            html.Button(id='submit-button-2', n_clicks=0, children='Fetch some news'),\n",
    "            \n",
    "            dcc.Graph(id='graph-2'),\n",
    "            \n",
    "            dt.DataTable(id='table',\n",
    "                         columns=[{'name':'Content', 'id':'Content'},\n",
    "                                  {'name':'Prediction', 'id':'Prediction'},\n",
    "                                  {'name':'Confidence', 'id':'Confidence'}],\n",
    "                         page_size=5,\n",
    "                         style_data={'whiteSpace':'normal',\n",
    "                                     'height':'auto'},\n",
    "                         style_cell={'textAlign':'left'}\n",
    "            )],\n",
    "            style={'width': '45%', 'display': 'inline-block', 'textAlign': 'center', 'verticalAlign': 'top'}),\n",
    "\n",
    "    ]),\n",
    "    \n",
    "])\n",
    "\n",
    "# callbacks\n",
    "\n",
    "@app.callback(\n",
    "    Output('output-1', 'children'),\n",
    "    Output('image-1', 'children'),\n",
    "    Input('submit-button-1', 'n_clicks'),\n",
    "    State('input-state', 'value')\n",
    ")\n",
    "def predict_from_input(n_clicks, input_value):\n",
    "    \n",
    "    if n_clicks == 0:\n",
    "        return f\"Try me! I'll surprise ya.\", ''\n",
    "    else:\n",
    "        df = pd.DataFrame({'Content': input_value}, index=[0])\n",
    "        df = clean_text_and_predict(df)\n",
    "        pred = df.loc[0, 'Prediction']\n",
    "        fname = glob.glob(f'/Users/michele/github/Guardian_news_classification/assets/{pred}.*')[0].split('/')[-1]\n",
    "        src = app.get_asset_url(f\"{fname}\")\n",
    "        img = html.Img(src=src, style={'width':'60%', 'height':'60%'})\n",
    "        if pred == 'not sure':\n",
    "            return \"I'm not sure this time...\", img\n",
    "        else:\n",
    "            return f\"Are you perhaps talking about {pred}?\", img\n",
    "\n",
    "#\n",
    "@app.callback(\n",
    "   Output('table', 'data'),\n",
    "   Output('graph-2', 'figure'),\n",
    "   Input('submit-button-2', 'n_clicks')\n",
    ")\n",
    "def scrape_and_predict(n_clicks):\n",
    "    \n",
    "    if n_clicks == 0:\n",
    "        return [{}], px.pie(values=[1], names=[''], hole=.3)\n",
    "    \n",
    "    # get dataframe\n",
    "    df = get_text()\n",
    "    df = clean_text_and_predict(df)\n",
    "\n",
    "    # update table\n",
    "    df['Content'] = df['Content'].str.slice(stop=150)\n",
    "    df['Content'] = df['Content'].astype(str) + '...'\n",
    "    records = df[['Content', 'Prediction', 'Confidence']].to_dict('records')\n",
    "    \n",
    "    # update pie chart\n",
    "    counts = df['Prediction'].value_counts()\n",
    "    names, values = list(zip(*[(name, count) for name, count in counts.items()]))\n",
    "    fig = px.pie(df, values=values, names=names, hole=.3)\n",
    "    \n",
    "    return records, fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "e1d7f245",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dash is running on http://127.0.0.1:8050/\n",
      "\n",
      "Dash is running on http://127.0.0.1:8050/\n",
      "\n",
      "Dash is running on http://127.0.0.1:8050/\n",
      "\n",
      "Dash is running on http://127.0.0.1:8050/\n",
      "\n",
      "Dash is running on http://127.0.0.1:8050/\n",
      "\n",
      "Dash is running on http://127.0.0.1:8050/\n",
      "\n",
      "Dash is running on http://127.0.0.1:8050/\n",
      "\n",
      "Dash is running on http://127.0.0.1:8050/\n",
      "\n",
      "Dash is running on http://127.0.0.1:8050/\n",
      "\n",
      "Dash is running on http://127.0.0.1:8050/\n",
      "\n",
      "Dash is running on http://127.0.0.1:8050/\n",
      "\n",
      "Dash is running on http://127.0.0.1:8050/\n",
      "\n",
      "Dash is running on http://127.0.0.1:8050/\n",
      "\n",
      "Dash is running on http://127.0.0.1:8050/\n",
      "\n",
      "Dash is running on http://127.0.0.1:8050/\n",
      "\n",
      "Dash is running on http://127.0.0.1:8050/\n",
      "\n",
      "Dash is running on http://127.0.0.1:8050/\n",
      "\n",
      "Dash is running on http://127.0.0.1:8050/\n",
      "\n",
      "Dash is running on http://127.0.0.1:8050/\n",
      "\n",
      "Dash is running on http://127.0.0.1:8050/\n",
      "\n",
      "Dash is running on http://127.0.0.1:8050/\n",
      "\n",
      "Dash is running on http://127.0.0.1:8050/\n",
      "\n",
      "Dash is running on http://127.0.0.1:8050/\n",
      "\n",
      "Dash is running on http://127.0.0.1:8050/\n",
      "\n",
      "Dash is running on http://127.0.0.1:8050/\n",
      "\n",
      "Dash is running on http://127.0.0.1:8050/\n",
      "\n",
      "Dash is running on http://127.0.0.1:8050/\n",
      "\n",
      "Dash is running on http://127.0.0.1:8050/\n",
      "\n",
      " * Serving Flask app \"__main__\" (lazy loading)\n",
      " * Environment: production\n",
      "\u001b[31m   WARNING: This is a development server. Do not use it in a production deployment.\u001b[0m\n",
      "\u001b[2m   Use a production WSGI server instead.\u001b[0m\n",
      " * Debug mode: off\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " * Running on http://127.0.0.1:8050/ (Press CTRL+C to quit)\n",
      "127.0.0.1 - - [01/Oct/2021 17:16:34] \"GET / HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [01/Oct/2021 17:16:37] \"GET /_dash-layout HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [01/Oct/2021 17:16:37] \"GET /_dash-dependencies HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [01/Oct/2021 17:16:37] \"GET /_favicon.ico?v=1.19.0 HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [01/Oct/2021 17:16:37] \"POST /_dash-update-component HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [01/Oct/2021 17:16:37] \"POST /_dash-update-component HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [01/Oct/2021 17:16:39] \"POST /_dash-update-component HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [01/Oct/2021 17:16:44] \"POST /_dash-update-component HTTP/1.1\" 200 -\n"
     ]
    }
   ],
   "source": [
    "app.run_server(debug=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0c0b310",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
