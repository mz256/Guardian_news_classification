{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1966bd82",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, time\n",
    "import requests\n",
    "import pickle\n",
    "import spacy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from pprint import pprint\n",
    "\n",
    "apikey = os.getenv('GUARDIAN_APIKEY')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf8de8ce",
   "metadata": {},
   "source": [
    "## Scraping the inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0afe9d2",
   "metadata": {},
   "source": [
    "Re-purpose the code from dataset creation to scrape just 5 new articles, this time from any section of the Guardian."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c67f3bc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions\n",
    "def get_results(base_url, params):\n",
    "    \n",
    "    results = []\n",
    "    try:\n",
    "        r = requests.get(base_url, params)\n",
    "        r.raise_for_status()\n",
    "    except requests.exceptions.RequestException as err:\n",
    "        raise SystemExit(err)\n",
    "    \n",
    "    data = r.json()\n",
    "    results.extend(data['response']['results'])\n",
    "    return results\n",
    "\n",
    "\n",
    "def results_to_html(results):\n",
    "    \n",
    "    # grab urls, write to file\n",
    "    urls = [result['webUrl'] for result in results]\n",
    "\n",
    "    # retrieve HTML from urls\n",
    "    html_files = {}\n",
    "    while len(html_files) < len(urls): \n",
    "        \n",
    "        for i, url in enumerate(urls):\n",
    "            if i not in html_files:\n",
    "                try:\n",
    "                    file = requests.get(url)\n",
    "                    file.raise_for_status()\n",
    "                    html_files[i] = file\n",
    "                except requests.exceptions.RequestException as err:\n",
    "                    time.sleep(2)\n",
    "    \n",
    "    return html_files\n",
    "        \n",
    "        \n",
    "def html_to_text(html_files):\n",
    "    \n",
    "    all_texts = []\n",
    "    \n",
    "    for file_id, file in html_files.items():\n",
    "        soup = BeautifulSoup(file.content, 'html.parser')\n",
    "        body = soup.find_all('div', class_='article-body-commercial-selector')\n",
    "        if len(body) == 1:\n",
    "            ps = body[0].find_all('p')\n",
    "        par_list = [p.text for p in ps]\n",
    "        text = \" \".join(par_list)\n",
    "        text = text.replace('\\xa0', ' ')\n",
    "        if not (text == ''):\n",
    "            all_texts.append(text)\n",
    "        \n",
    "    return all_texts\n",
    "\n",
    "# Main function to scrape and collect news articles for inference\n",
    "def get_text():\n",
    "    \n",
    "    API_ENDPOINT = \"http://content.guardianapis.com/search\"\n",
    "    my_params = {\n",
    "        'api-key': apikey,\n",
    "        'order-by': 'relevance', \n",
    "        'from-date': \"2020-1-1\",\n",
    "        'page-size': 5,\n",
    "    }\n",
    "    \n",
    "    results = get_results(API_ENDPOINT, my_params)\n",
    "    html_files = results_to_html(results)\n",
    "    texts = html_to_text(html_files)\n",
    "    df = pd.DataFrame({'Content': texts})\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3b7ace13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>There are still plenty of unanswered questions...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Margaret Nolan, the actor best known for appea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Morning everyone: I’m Martin Farrer and these ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Vlad has been setting cryptics and Genius puzz...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Marcus Rashford has revealed a pep talk from t...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Content\n",
       "0  There are still plenty of unanswered questions...\n",
       "1  Margaret Nolan, the actor best known for appea...\n",
       "2  Morning everyone: I’m Martin Farrer and these ...\n",
       "3  Vlad has been setting cryptics and Genius puzz...\n",
       "4  Marcus Rashford has revealed a pep talk from t..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = get_text()\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "becd3729",
   "metadata": {},
   "source": [
    "## From inputs to category prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed1e51c9",
   "metadata": {},
   "source": [
    "Load trained classifier and TF-IDF vectoriser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1780b8f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"models/best_svm.pickle\", 'rb') as model:\n",
    "    svc = pickle.load(model)\n",
    "    \n",
    "with open(\"processed/tfidf_vectoriser.pickle\", 'rb') as f:\n",
    "    vectoriser = pickle.load(f)\n",
    "    \n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a58ed2b",
   "metadata": {},
   "source": [
    "Create dictionary to convert predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ece946de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'environment', 1: 'business', 2: 'film', 3: 'culture', 4: 'education', 5: 'other'}\n"
     ]
    }
   ],
   "source": [
    "# create dictionary\n",
    "sections = ['environment', 'business', 'film', 'culture', 'education']\n",
    "d = {i: section for i, section in enumerate(sections)}\n",
    "\n",
    "# add the \"other\" category when model is unsure\n",
    "d.update({5: 'other'})\n",
    "print(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8d718c0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions\n",
    "def keep_token(t):\n",
    "    \"\"\"Decide whether to keep a token\"\"\"\n",
    "    return (t.is_alpha and not (t.is_space or t.is_punct or t.is_stop))\n",
    "\n",
    "\n",
    "def lemmatised_string(doc):\n",
    "    \"\"\"Lemmatise remaining tokens\"\"\"\n",
    "    return \" \".join(t.lemma_ for t in doc if keep_token(t))\n",
    "\n",
    "\n",
    "def predict_category(features):\n",
    "    initial_preds = svc.predict(features)\n",
    "    probs = svc.predict_proba(features).max(axis=1)\n",
    "    preds = []\n",
    "    \n",
    "    for prob, in_pred in zip(probs, initial_preds):\n",
    "        if prob > 0.7:\n",
    "            preds.append(in_pred)\n",
    "        else:\n",
    "            preds.append(5)\n",
    "    return [d[pred] for pred in preds]\n",
    "\n",
    "# Main function \n",
    "def clean_text_and_predict(df_0):\n",
    "    \n",
    "    df = df_0.copy()\n",
    "    df['Content_parsed'] = df['Content'].str.lower()\n",
    "    df['Content_parsed'] = df['Content_parsed'].str.strip()\n",
    "    \n",
    "    # parse and clean articles\n",
    "    docs = list(nlp.pipe(df['Content_parsed'], disable=['tok2vec','ner','tagger','parser']))\n",
    "    df['Content_parsed'] = [lemmatised_string(doc) for doc in docs]\n",
    "    \n",
    "    # numericalise with learnt transformer\n",
    "    features = vectoriser.transform(df['Content_parsed']).toarray()\n",
    "    \n",
    "    df['Prediction'] = predict_category(features)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "29523ae6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Content</th>\n",
       "      <th>Content_parsed</th>\n",
       "      <th>Prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>There are still plenty of unanswered questions...</td>\n",
       "      <td>plenty unanswered question follow tuesday news...</td>\n",
       "      <td>other</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Margaret Nolan, the actor best known for appea...</td>\n",
       "      <td>margaret nolan actor well know appear title se...</td>\n",
       "      <td>film</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Morning everyone: I’m Martin Farrer and these ...</td>\n",
       "      <td>morning martin farrer story start week boris j...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Vlad has been setting cryptics and Genius puzz...</td>\n",
       "      <td>vlad set cryptics genius puzzle guardian know ...</td>\n",
       "      <td>culture</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Marcus Rashford has revealed a pep talk from t...</td>\n",
       "      <td>marcus rashford reveal pep talk manchester uni...</td>\n",
       "      <td>other</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Content  \\\n",
       "0  There are still plenty of unanswered questions...   \n",
       "1  Margaret Nolan, the actor best known for appea...   \n",
       "2  Morning everyone: I’m Martin Farrer and these ...   \n",
       "3  Vlad has been setting cryptics and Genius puzz...   \n",
       "4  Marcus Rashford has revealed a pep talk from t...   \n",
       "\n",
       "                                      Content_parsed Prediction  \n",
       "0  plenty unanswered question follow tuesday news...      other  \n",
       "1  margaret nolan actor well know appear title se...       film  \n",
       "2  morning martin farrer story start week boris j...   business  \n",
       "3  vlad set cryptics genius puzzle guardian know ...    culture  \n",
       "4  marcus rashford reveal pep talk manchester uni...      other  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_text_and_predict(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4885b4b2",
   "metadata": {},
   "source": [
    "## Create app in Dash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "24a7a473",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dash\n",
    "import dash_core_components as dcc\n",
    "import dash_html_components as html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "afa4388c",
   "metadata": {},
   "outputs": [],
   "source": [
    "app = dash.Dash(__name__)\n",
    "\n",
    "app.layout = html.Div(children=[\n",
    "    \n",
    "    html.H1(children='My app'),\n",
    "    \n",
    "    \n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e1d7f245",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dash is running on http://127.0.0.1:8050/\n",
      "\n",
      "Dash is running on http://127.0.0.1:8050/\n",
      "\n",
      " * Serving Flask app \"__main__\" (lazy loading)\n",
      " * Environment: production\n",
      "\u001b[31m   WARNING: This is a development server. Do not use it in a production deployment.\u001b[0m\n",
      "\u001b[2m   Use a production WSGI server instead.\u001b[0m\n",
      " * Debug mode: off\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " * Running on http://127.0.0.1:8050/ (Press CTRL+C to quit)\n",
      "127.0.0.1 - - [27/Sep/2021 16:34:29] \"GET / HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [27/Sep/2021 16:34:29] \"GET /_dash-component-suites/dash_core_components/plotly-1.v1_3_1m1576595950.50.1.min.js HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [27/Sep/2021 16:34:30] \"GET /_dash-layout HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [27/Sep/2021 16:34:30] \"GET /_dash-dependencies HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [27/Sep/2021 16:47:30] \"GET / HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [27/Sep/2021 16:47:31] \"GET /_dash-dependencies HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [27/Sep/2021 16:47:31] \"GET /_dash-layout HTTP/1.1\" 200 -\n"
     ]
    }
   ],
   "source": [
    "app.run_server(debug=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91c45ef4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
