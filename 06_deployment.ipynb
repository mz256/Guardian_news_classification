{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1966bd82",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, time\n",
    "import requests\n",
    "import pickle\n",
    "import spacy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from pprint import pprint\n",
    "\n",
    "apikey = os.getenv('GUARDIAN_APIKEY')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf8de8ce",
   "metadata": {},
   "source": [
    "## Scraping the inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0afe9d2",
   "metadata": {},
   "source": [
    "Re-purpose the code from dataset creation to scrape just 5 new articles, this time from any section of the Guardian."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c67f3bc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions\n",
    "def get_results(base_url, params):\n",
    "    \n",
    "    results = []\n",
    "    try:\n",
    "        r = requests.get(base_url, params)\n",
    "        r.raise_for_status()\n",
    "    except requests.exceptions.RequestException as err:\n",
    "        raise SystemExit(err)\n",
    "    \n",
    "    data = r.json()\n",
    "    results.extend(data['response']['results'])\n",
    "    return results\n",
    "\n",
    "\n",
    "def results_to_html(results):\n",
    "    \n",
    "    # grab urls, write to file\n",
    "    urls = [result['webUrl'] for result in results]\n",
    "\n",
    "    # retrieve HTML from urls\n",
    "    html_files = {}\n",
    "    while len(html_files) < len(urls): \n",
    "        \n",
    "        for i, url in enumerate(urls):\n",
    "            if i not in html_files:\n",
    "                try:\n",
    "                    file = requests.get(url)\n",
    "                    file.raise_for_status()\n",
    "                    html_files[i] = file\n",
    "                except requests.exceptions.RequestException as err:\n",
    "                    time.sleep(2)\n",
    "    \n",
    "    return html_files\n",
    "        \n",
    "        \n",
    "def html_to_text(html_files):\n",
    "    \n",
    "    all_texts = []\n",
    "    \n",
    "    for file_id, file in html_files.items():\n",
    "        soup = BeautifulSoup(file.content, 'html.parser')\n",
    "        body = soup.find_all('div', class_='article-body-commercial-selector')\n",
    "        if len(body) == 1:\n",
    "            ps = body[0].find_all('p')\n",
    "        par_list = [p.text for p in ps]\n",
    "        text = \" \".join(par_list)\n",
    "        text = text.replace('\\xa0', ' ')\n",
    "        if not (text == ''):\n",
    "            all_texts.append(text)\n",
    "        \n",
    "    return all_texts\n",
    "\n",
    "# Main function to scrape and collect news articles for inference\n",
    "def get_text():\n",
    "    \n",
    "    API_ENDPOINT = \"http://content.guardianapis.com/search\"\n",
    "    my_params = {\n",
    "        'api-key': apikey,\n",
    "        'order-by': 'relevance', \n",
    "        'from-date': \"2020-1-1\",\n",
    "        'page-size': 5,\n",
    "    }\n",
    "    \n",
    "    results = get_results(API_ENDPOINT, my_params)\n",
    "    html_files = results_to_html(results)\n",
    "    texts = html_to_text(html_files)\n",
    "    df = pd.DataFrame({'Content': texts})\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b7ace13",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = get_text()\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "becd3729",
   "metadata": {},
   "source": [
    "## From inputs to category prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed1e51c9",
   "metadata": {},
   "source": [
    "Load trained classifier and TF-IDF vectoriser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1780b8f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"models/best_svm.pickle\", 'rb') as model:\n",
    "    svc = pickle.load(model)\n",
    "    \n",
    "with open(\"processed/tfidf_vectoriser.pickle\", 'rb') as f:\n",
    "    vectoriser = pickle.load(f)\n",
    "    \n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a58ed2b",
   "metadata": {},
   "source": [
    "Create dictionary to convert predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ece946de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dictionary\n",
    "sections = ['environment', 'business', 'film', 'culture', 'education']\n",
    "d = {i: section for i, section in enumerate(sections)}\n",
    "\n",
    "# add the \"other\" category when model is unsure\n",
    "d.update({5: \"other stuff\"})\n",
    "print(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d718c0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions\n",
    "def keep_token(t):\n",
    "    \"\"\"Decide whether to keep a token\"\"\"\n",
    "    return (t.is_alpha and not (t.is_space or t.is_punct or t.is_stop))\n",
    "\n",
    "\n",
    "def lemmatised_string(doc):\n",
    "    \"\"\"Lemmatise remaining tokens\"\"\"\n",
    "    return \" \".join(t.lemma_ for t in doc if keep_token(t))\n",
    "\n",
    "\n",
    "def predict_category(features):\n",
    "    initial_preds = svc.predict(features)\n",
    "    probs = svc.predict_proba(features).max(axis=1)\n",
    "    preds = []\n",
    "    \n",
    "    for prob, in_pred in zip(probs, initial_preds):\n",
    "        if prob > 0.7:\n",
    "            preds.append(in_pred)\n",
    "        else:\n",
    "            preds.append(5)\n",
    "    return [d[pred] for pred in preds]\n",
    "\n",
    "# Main function \n",
    "def clean_text_and_predict(df_0):\n",
    "    \n",
    "    df = df_0.copy()\n",
    "    df['Content_parsed'] = df['Content'].str.lower()\n",
    "    df['Content_parsed'] = df['Content_parsed'].str.strip()\n",
    "    \n",
    "    # parse and clean articles\n",
    "    docs = list(nlp.pipe(df['Content_parsed'], disable=['tok2vec','ner','tagger','parser']))\n",
    "    df['Content_parsed'] = [lemmatised_string(doc) for doc in docs]\n",
    "    \n",
    "    # numericalise with learnt transformer\n",
    "    features = vectoriser.transform(df['Content_parsed']).toarray()\n",
    "    \n",
    "    df['Prediction'] = predict_category(features)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29523ae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_text_and_predict(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4885b4b2",
   "metadata": {},
   "source": [
    "## Create app in Dash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24a7a473",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dash\n",
    "import dash_core_components as dcc\n",
    "import dash_html_components as html\n",
    "from dash.dependencies import Input, Output, State"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afa4388c",
   "metadata": {},
   "outputs": [],
   "source": [
    "app = dash.Dash(__name__)\n",
    "\n",
    "colors = {\n",
    "    'background': '#FFFFFF',#'#212F3D',\n",
    "    'text': '#000000' #'#FBFCFC'\n",
    "}\n",
    "\n",
    "app.layout = html.Div(style={'backgroundColor': colors['background']}, children=[\n",
    "    \n",
    "    #########\n",
    "    # TITLE\n",
    "    #########\n",
    "    html.H1(\n",
    "        children='Wanna see if I can guess...',\n",
    "        style={\n",
    "            'textAlign': 'center',\n",
    "            'color': colors['text']\n",
    "        }\n",
    "    ),\n",
    "    \n",
    "    # space\n",
    "    html.Br(),\n",
    "    \n",
    "    #########\n",
    "    # DECIDE-WHAT-TO-DO SECTION\n",
    "    #########\n",
    "    html.Div([\n",
    "        \n",
    "        # left panel (prediction on user input)\n",
    "        html.Div([\n",
    "            html.H3(\"...what you are writing about?\"),\n",
    "            html.Div([\n",
    "                dcc.Input(id='input-state', value='write here', type='text'),\n",
    "                html.Button(id='submit-button-1', n_clicks=0, children='Let me guess')\n",
    "            ]),\n",
    "        ],\n",
    "        style={'width': '49%', 'display': 'inline-block', 'textAlign': 'center'}),\n",
    "        \n",
    "        # right panel (prediction on scraped news)\n",
    "        html.Div([\n",
    "            html.H3(\"...what the Guardian is writing about?\"),\n",
    "            html.Div([\n",
    "                html.Button(id='submit-button-2', n_clicks=0, children='Fetch some news')            \n",
    "            ]),\n",
    "        ],\n",
    "        style={'width': '49%', 'display': 'inline-block', 'textAlign': 'center'})\n",
    "    ]),\n",
    "    \n",
    "    # space\n",
    "    html.Br(),\n",
    "    \n",
    "    #########\n",
    "    # OUTPUTS AND GRAPHS\n",
    "    #########\n",
    "    html.Div([\n",
    "        \n",
    "        # left panel\n",
    "        html.Div([\n",
    "            html.Div(id='output-1'),\n",
    "            dcc.Graph(id='graph-1')],\n",
    "            style={'width': '49%', 'display': 'inline-block', 'textAlign': 'center', 'float': 'left'}\n",
    "        ),\n",
    "\n",
    "        # right panel\n",
    "        html.Div([\n",
    "            html.Div(id='output-2'),\n",
    "            dcc.Graph(id='graph-2')],\n",
    "            style={'width': '49%','display': 'inline-block', 'textAlign': 'center', 'float': 'right'}\n",
    "        ),\n",
    "    ])\n",
    "    # storing temporary values\n",
    "    #html.Div(id='temp', style={'display': 'none'})\n",
    "    \n",
    "    \n",
    "])\n",
    "\n",
    "# callbacks\n",
    "@app.callback(\n",
    "    Output('output-1', 'children'),\n",
    "    Input('submit-button-1', 'n_clicks'),\n",
    "    State('input-state', 'value')\n",
    ")\n",
    "def predict_from_input(n_clicks, input_value):\n",
    "    \n",
    "    if n_clicks == 0:\n",
    "        return f\"Try me! I'll surprise ya.\"\n",
    "    else:\n",
    "        df = pd.DataFrame({'Content': input_value}, index=[0])\n",
    "        df = clean_text_and_predict(df)\n",
    "        return f\"Are you perhaps talking about...{df.loc[0, 'Prediction']}?\"\n",
    "\n",
    "#\n",
    "@app.callback(\n",
    "    Output('output-2', 'children'),\n",
    "    Output('graph-2', 'figure'),\n",
    "    Input('submit-button-2', 'n_clicks')\n",
    ")\n",
    "def scrape_and_predict(n_clicks):\n",
    "    \n",
    "    if n_clicks == 0:\n",
    "        return \"\"\n",
    "    df = get_text()\n",
    "    df = clean_text_and_predict(df)\n",
    "    \n",
    "    figure = {\n",
    "        'data' : [\n",
    "            {'values': [1,2,3,4,5,6],\n",
    "             'labels': list(d.values()),\n",
    "             'type': 'pie'}\n",
    "        ]\n",
    "    }\n",
    "    return f\"{df['Prediction'].tolist()}\", figure\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1d7f245",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "app.run_server(debug=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91c45ef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = 'hello football game soccer'\n",
    "df1 = pd.DataFrame({'Content': inp}, index=[0])\n",
    "df1 = clean_text_and_predict(df1)\n",
    "\n",
    "print(df1.loc[0, 'Prediction'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "727ea5c9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
