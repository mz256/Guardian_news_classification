{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9e8b4267",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, time\n",
    "import requests\n",
    "import random\n",
    "import pickle\n",
    "import spacy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from pprint import pprint\n",
    "\n",
    "import dash\n",
    "import dash_table as dt\n",
    "import dash_core_components as dcc\n",
    "import dash_html_components as html\n",
    "from dash.dependencies import Input, Output, State\n",
    "import plotly.express as px\n",
    "\n",
    "from PIL import Image\n",
    "import glob\n",
    "\n",
    "apikey = os.getenv('GUARDIAN_APIKEY')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc4d23ba",
   "metadata": {},
   "source": [
    "## Scraping the inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04c514a7",
   "metadata": {},
   "source": [
    "Re-purpose the code from dataset creation to scrape just 5 new articles, this time from any section of the Guardian."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "531bc4bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions\n",
    "def get_results(base_url, params):\n",
    "    \n",
    "    results = []\n",
    "    try:\n",
    "        r = requests.get(base_url, params)\n",
    "        r.raise_for_status()\n",
    "    except requests.exceptions.RequestException as err:\n",
    "        raise SystemExit(err)\n",
    "    \n",
    "    data = r.json()\n",
    "    results.extend(data['response']['results'])\n",
    "    return results\n",
    "\n",
    "\n",
    "def results_to_html(results):\n",
    "    \n",
    "    # grab urls, write to file\n",
    "    urls = [result['webUrl'] for result in results]\n",
    "\n",
    "    # retrieve HTML from urls\n",
    "    html_files = {}\n",
    "    while len(html_files) < len(urls): \n",
    "        \n",
    "        for i, url in enumerate(urls):\n",
    "            if i not in html_files:\n",
    "                try:\n",
    "                    file = requests.get(url, timeout=0.5)\n",
    "                    file.raise_for_status()\n",
    "                    html_files[i] = file\n",
    "                except requests.exceptions.RequestException as err:\n",
    "                    # print(err)\n",
    "                    time.sleep(2)\n",
    "    \n",
    "    return html_files\n",
    "        \n",
    "        \n",
    "def html_to_text(html_files):\n",
    "    \n",
    "    all_texts = []\n",
    "    \n",
    "    for file_id, file in html_files.items():\n",
    "        soup = BeautifulSoup(file.content, 'html.parser')\n",
    "        body = soup.find_all('div', class_='article-body-commercial-selector')\n",
    "        if len(body) == 1:\n",
    "            ps = body[0].find_all('p')\n",
    "        par_list = [p.text for p in ps]\n",
    "        text = \" \".join(par_list)\n",
    "        text = text.replace('\\xa0', ' ')\n",
    "        if not (text == ''):\n",
    "            all_texts.append(text)\n",
    "        \n",
    "    return all_texts\n",
    "\n",
    "# Main function to scrape and collect news articles for inference\n",
    "def get_text():\n",
    "    \n",
    "    API_ENDPOINT = \"http://content.guardianapis.com/search\"\n",
    "    # pick a random page, to display different results\n",
    "    page_number = random.randint(0,15)\n",
    "    my_params = {\n",
    "        'api-key': apikey,\n",
    "        'order-by': 'relevance', \n",
    "        'from-date': \"2020-1-1\",\n",
    "        'page-size': 10,\n",
    "        'page': page_number\n",
    "    }\n",
    "    \n",
    "    results = get_results(API_ENDPOINT, my_params)\n",
    "    html_files = results_to_html(results)\n",
    "    texts = html_to_text(html_files)\n",
    "    df = pd.DataFrame({'Content': texts})\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f1c3d466",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>All adults in the UK should have been offered ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Emerging from Holy Family hospital in New Delh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A consortium that includes high street giant N...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Labour has urged ministers to make May’s elect...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>There was a sense of excitement at the Garden ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Three in four Australians agree that Scott Mor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>The public will not see Donald Trump’s White H...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>The Department of Home Affairs mistakenly sent...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Vogue will publish a limited print edition of ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Companies that make dangerous building materia...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Content\n",
       "0  All adults in the UK should have been offered ...\n",
       "1  Emerging from Holy Family hospital in New Delh...\n",
       "2  A consortium that includes high street giant N...\n",
       "3  Labour has urged ministers to make May’s elect...\n",
       "4  There was a sense of excitement at the Garden ...\n",
       "5  Three in four Australians agree that Scott Mor...\n",
       "6  The public will not see Donald Trump’s White H...\n",
       "7  The Department of Home Affairs mistakenly sent...\n",
       "8  Vogue will publish a limited print edition of ...\n",
       "9  Companies that make dangerous building materia..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = get_text()\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7628d4f1",
   "metadata": {},
   "source": [
    "## From inputs to category prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e58f5a95",
   "metadata": {},
   "source": [
    "Load trained classifier and TF-IDF vectoriser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f4899f66",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"models/best_svm.pickle\", 'rb') as model:\n",
    "    svc = pickle.load(model)\n",
    "    \n",
    "with open(\"processed/tfidf_vectoriser.pickle\", 'rb') as f:\n",
    "    vectoriser = pickle.load(f)\n",
    "    \n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31b391cc",
   "metadata": {},
   "source": [
    "Create dictionary to convert predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dd3cb9d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'environment', 1: 'business', 2: 'film', 3: 'culture', 4: 'education', 5: 'not sure'}\n"
     ]
    }
   ],
   "source": [
    "# create dictionary\n",
    "sections = ['environment', 'business', 'film', 'culture', 'education']\n",
    "d = {i: section for i, section in enumerate(sections)}\n",
    "\n",
    "# add the \"other\" category when model is unsure\n",
    "d.update({5: \"not sure\"})\n",
    "print(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a567fe4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions\n",
    "def keep_token(t):\n",
    "    \"\"\"Decide whether to keep a token\"\"\"\n",
    "    return (t.is_alpha and not (t.is_space or t.is_punct or t.is_stop))\n",
    "\n",
    "\n",
    "def lemmatised_string(doc):\n",
    "    \"\"\"Lemmatise remaining tokens\"\"\"\n",
    "    return \" \".join(t.lemma_ for t in doc if keep_token(t))\n",
    "\n",
    "\n",
    "def predict_category(features):\n",
    "    initial_preds = svc.predict(features)\n",
    "    probs = svc.predict_proba(features).max(axis=1)\n",
    "    preds = []\n",
    "    \n",
    "    for prob, in_pred in zip(probs, initial_preds):\n",
    "        if prob > 0.5:\n",
    "            preds.append(in_pred)\n",
    "        else:\n",
    "            preds.append(5)\n",
    "    return [d[pred] for pred in preds], probs\n",
    "\n",
    "# Main function \n",
    "def clean_text_and_predict(df_0):\n",
    "    \n",
    "    df = df_0.copy()\n",
    "    df['Content_parsed'] = df['Content'].str.lower()\n",
    "    df['Content_parsed'] = df['Content_parsed'].str.strip()\n",
    "    \n",
    "    # parse and clean articles\n",
    "    docs = list(nlp.pipe(df['Content_parsed'], disable=['tok2vec','ner','tagger','parser']))\n",
    "    df['Content_parsed'] = [lemmatised_string(doc) for doc in docs]\n",
    "    \n",
    "    # numericalise with learnt transformer\n",
    "    features = vectoriser.transform(df['Content_parsed']).toarray()\n",
    "    \n",
    "    preds, probs = predict_category(features)\n",
    "    df['Prediction'] = preds\n",
    "    df['Confidence'] = probs\n",
    "    df['Confidence'] = (df['Confidence']*100).map('{:,.0f}%'.format)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d6f1e771",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Content</th>\n",
       "      <th>Content_parsed</th>\n",
       "      <th>Prediction</th>\n",
       "      <th>Confidence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>All adults in the UK should have been offered ...</td>\n",
       "      <td>adult uk offer covid vaccination september dom...</td>\n",
       "      <td>business</td>\n",
       "      <td>74%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Emerging from Holy Family hospital in New Delh...</td>\n",
       "      <td>emerge holy family hospital new delhi ram verm...</td>\n",
       "      <td>not sure</td>\n",
       "      <td>50%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A consortium that includes high street giant N...</td>\n",
       "      <td>consortium include high street giant think pol...</td>\n",
       "      <td>business</td>\n",
       "      <td>99%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Labour has urged ministers to make May’s elect...</td>\n",
       "      <td>labour urge minister election england covid se...</td>\n",
       "      <td>environment</td>\n",
       "      <td>64%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>There was a sense of excitement at the Garden ...</td>\n",
       "      <td>sense excitement garden house devon weekend ga...</td>\n",
       "      <td>culture</td>\n",
       "      <td>68%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Three in four Australians agree that Scott Mor...</td>\n",
       "      <td>australians agree scott morrison publicly rebu...</td>\n",
       "      <td>culture</td>\n",
       "      <td>76%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>The public will not see Donald Trump’s White H...</td>\n",
       "      <td>public donald trump white house record year gr...</td>\n",
       "      <td>culture</td>\n",
       "      <td>84%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>The Department of Home Affairs mistakenly sent...</td>\n",
       "      <td>department home affair mistakenly send outdate...</td>\n",
       "      <td>business</td>\n",
       "      <td>71%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Vogue will publish a limited print edition of ...</td>\n",
       "      <td>vogue publish limit print edition february iss...</td>\n",
       "      <td>culture</td>\n",
       "      <td>54%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Companies that make dangerous building materia...</td>\n",
       "      <td>company dangerous build material grenfell towe...</td>\n",
       "      <td>business</td>\n",
       "      <td>80%</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Content  \\\n",
       "0  All adults in the UK should have been offered ...   \n",
       "1  Emerging from Holy Family hospital in New Delh...   \n",
       "2  A consortium that includes high street giant N...   \n",
       "3  Labour has urged ministers to make May’s elect...   \n",
       "4  There was a sense of excitement at the Garden ...   \n",
       "5  Three in four Australians agree that Scott Mor...   \n",
       "6  The public will not see Donald Trump’s White H...   \n",
       "7  The Department of Home Affairs mistakenly sent...   \n",
       "8  Vogue will publish a limited print edition of ...   \n",
       "9  Companies that make dangerous building materia...   \n",
       "\n",
       "                                      Content_parsed   Prediction Confidence  \n",
       "0  adult uk offer covid vaccination september dom...     business        74%  \n",
       "1  emerge holy family hospital new delhi ram verm...     not sure        50%  \n",
       "2  consortium include high street giant think pol...     business        99%  \n",
       "3  labour urge minister election england covid se...  environment        64%  \n",
       "4  sense excitement garden house devon weekend ga...      culture        68%  \n",
       "5  australians agree scott morrison publicly rebu...      culture        76%  \n",
       "6  public donald trump white house record year gr...      culture        84%  \n",
       "7  department home affair mistakenly send outdate...     business        71%  \n",
       "8  vogue publish limit print edition february iss...      culture        54%  \n",
       "9  company dangerous build material grenfell towe...     business        80%  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_text_and_predict(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a4de028",
   "metadata": {},
   "source": [
    "## Create app in Dash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e3e3dc2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "external_stylesheets = ['https://codepen.io/chriddyp/pen/bWLwgP.css']\n",
    "\n",
    "app = dash.Dash(__name__, external_stylesheets=external_stylesheets)\n",
    "\n",
    "colors = {\n",
    "    'background': '#FFFFFF',#'#212F3D',\n",
    "    'text': '#000000' #'#FBFCFC'\n",
    "}\n",
    "\n",
    "#current_dir = os.getcwd() + \"/images/\"\n",
    "\n",
    "markdown = \"\"\"\n",
    "This app predicts the topic of different bodies of text, either input by the user or news articles scraped from the Guardian.\n",
    "\"\"\"\n",
    "\n",
    "app.layout = html.Div(style={'backgroundColor': colors['background']}, children=[   \n",
    "\n",
    "    # title\n",
    "    html.H1(\n",
    "        children='Wanna see if I can guess...',\n",
    "        style={\n",
    "            'textAlign': 'center',\n",
    "            'color': colors['text']\n",
    "        }\n",
    "    ),\n",
    "    \n",
    "    # space\n",
    "    html.Br(),\n",
    "    \n",
    "    # main section\n",
    "    html.Div([\n",
    "        \n",
    "        # LEFT PANEL (prediction on user input)\n",
    "        html.Div([\n",
    "            html.H3(\"...what you are writing about?\"),\n",
    "            \n",
    "            dcc.Textarea(id='input-state',\n",
    "                         placeholder='Write here',\n",
    "                         style={'width': 500, 'height': 120}),\n",
    "            \n",
    "            html.Div([\n",
    "                html.Button(id='submit-button-1', n_clicks=0, children='Let me guess',\n",
    "                        style={'marginBottom':50})\n",
    "            ]),\n",
    "                       \n",
    "            html.H5(id='output-1'),\n",
    "            \n",
    "            html.Br(),\n",
    "            \n",
    "            html.Div(id='image-1', style={'marginTop': 100})\n",
    "        ],\n",
    "        style={'width': '45%', 'display': 'inline-block', 'textAlign': 'center', 'verticalAlign': 'top'}),\n",
    "        \n",
    "        \n",
    "        # RIGHT PANEL (prediction on scraped news)\n",
    "        html.Div([\n",
    "            html.H3(\"...what the Guardian is writing about?\"),\n",
    "            \n",
    "            html.Button(id='submit-button-2', n_clicks=0, children='Fetch some news'),\n",
    "            \n",
    "            dcc.Graph(id='graph-2'),\n",
    "            \n",
    "            dt.DataTable(id='table',\n",
    "                         columns=[{'name':'Content', 'id':'Content'},\n",
    "                                  {'name':'Prediction', 'id':'Prediction'},\n",
    "                                  {'name':'Confidence', 'id':'Confidence'}],\n",
    "                         page_size=5,\n",
    "                         style_data={'whiteSpace':'normal',\n",
    "                                     'height':'auto'},\n",
    "                         style_cell={'textAlign':'left'}\n",
    "            )],\n",
    "            style={'width': '45%', 'display': 'inline-block', 'textAlign': 'center', 'verticalAlign': 'top'}),\n",
    "\n",
    "    ]),\n",
    "    \n",
    "])\n",
    "\n",
    "# callbacks\n",
    "\n",
    "@app.callback(\n",
    "    Output('output-1', 'children'),\n",
    "    Output('image-1', 'children'),\n",
    "    Input('submit-button-1', 'n_clicks'),\n",
    "    State('input-state', 'value')\n",
    ")\n",
    "def predict_from_input(n_clicks, input_value):\n",
    "    \n",
    "    if n_clicks == 0:\n",
    "        return f\"Try me! I'll surprise ya.\", ''\n",
    "    else:\n",
    "        df = pd.DataFrame({'Content': input_value}, index=[0])\n",
    "        df = clean_text_and_predict(df)\n",
    "        pred = df.loc[0, 'Prediction']\n",
    "        fname = glob.glob(f'/Users/michele/github/Guardian_news_classification/assets/{pred}.*')[0].split('/')[-1]\n",
    "        src = app.get_asset_url(f\"{fname}\")\n",
    "        img = html.Img(src=src, style={'width':'60%', 'height':'60%'})\n",
    "        if pred == 'not sure':\n",
    "            return \"I'm not sure this time...\", img\n",
    "        else:\n",
    "            return f\"Are you perhaps talking about {pred}?\", img\n",
    "\n",
    "#\n",
    "@app.callback(\n",
    "   Output('table', 'data'),\n",
    "   Output('graph-2', 'figure'),\n",
    "   Input('submit-button-2', 'n_clicks')\n",
    ")\n",
    "def scrape_and_predict(n_clicks):\n",
    "    \n",
    "    if n_clicks == 0:\n",
    "        return [{}], px.pie(values=[1], names=[''], hole=.3)\n",
    "    \n",
    "    # get dataframe\n",
    "    df = get_text()\n",
    "    df = clean_text_and_predict(df)\n",
    "\n",
    "    # update table\n",
    "    df['Content'] = df['Content'].str.slice(stop=150)\n",
    "    df['Content'] = df['Content'].astype(str) + '...'\n",
    "    records = df[['Content', 'Prediction', 'Confidence']].to_dict('records')\n",
    "    \n",
    "    # update pie chart\n",
    "    counts = df['Prediction'].value_counts()\n",
    "    names, values = list(zip(*[(name, count) for name, count in counts.items()]))\n",
    "    fig = px.pie(df, values=values, names=names, hole=.3)\n",
    "    \n",
    "    return records, fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b29a64e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dash is running on http://127.0.0.1:8050/\n",
      "\n",
      " * Serving Flask app \"__main__\" (lazy loading)\n",
      " * Environment: production\n",
      "\u001b[31m   WARNING: This is a development server. Do not use it in a production deployment.\u001b[0m\n",
      "\u001b[2m   Use a production WSGI server instead.\u001b[0m\n",
      " * Debug mode: off\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " * Running on http://127.0.0.1:8050/ (Press CTRL+C to quit)\n"
     ]
    }
   ],
   "source": [
    "app.run_server(debug=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82a671cb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
