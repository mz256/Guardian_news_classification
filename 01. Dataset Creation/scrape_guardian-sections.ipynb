{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraping news articles with TheGuardianAPI and BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook creates a dataset for Natural Language Processing (NLP) by scraping news articles from the Guardian. \n",
    "\n",
    "First, it collects the location (urls) of the desired news articles using the Guardian Open Platform, specifically the content API endpoint. Then, it scrapes the text from each with the BeautifulSoup python library and saves it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try to obtain the text from a single article first, to get familiar with the API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, time\n",
    "import requests\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from pprint import pprint\n",
    "\n",
    "apikey = os.getenv('GUARDIAN_APIKEY')\n",
    "BASE_URL = \"http://content.guardianapis.com/search?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# query = \"China\"\n",
    "# query_fields = \"body\"\n",
    "section = 'environment'\n",
    "order_by = 'relevance'\n",
    "from_date = \"2020-1-1T00:00:00\"\n",
    "to_date = \"2021-9-22T00:00:00\"\n",
    "query_url = f\"{BASE_URL}&api-key={apikey}\" \\\n",
    "            f\"&section={section}\" \\\n",
    "            f\"&order-by={order_by}\" \\\n",
    "            f\"&from-date={from_date}\" # \\\n",
    "            # f\"&to-date={to_date}\" # \\\n",
    "            # f\"&show-fields=body\"\n",
    "\n",
    "# query_url "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = requests.get(query_url)\n",
    "print(f\"Status code: {r.status_code}\\n\")\n",
    "print(f\"Headers: {r.headers}\\n\")\n",
    "#pprint(r.json())\n",
    "r.json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the request was successful, and we printed the contents in pretty-printed JSON format for readability. We could demand that the field `body` be returned as well, as a potential shortcut to calling `requests` a second time on the individual article urls. This, however, has some subtle behaviour, so we will go for the traditional route. Now, let's see the url of the article we downloaded!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = r.json()['response']['results'][0]['webUrl']\n",
    "print(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we requests the article itself and parse it with BeautifulSoup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "article = requests.get(url)\n",
    "soup_article = BeautifulSoup(article.content, 'html.parser')\n",
    "# print(soup_article.prettify())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We list all `p` tags with the specified properties (i.e. class and position inside a certain `div`), we extract and collate the text. And we are done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "body = soup_article.find_all('div', class_='article-body-commercial-selector')\n",
    "ps = body[0].find_all('p', class_='dcr-s23rjr')\n",
    "par_list = [p.text for p in ps]\n",
    "final = \" \".join(par_list)\n",
    "# replace the Unicode-converted HTML entities\n",
    "final = final.replace('\\xa0', ' ')\n",
    "final"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I would imagine there should be a shortcut to all this, since the API can return the body in HTML if prompted (calling `show-fields=body` in the api call above). This, however, contains certain artifacts (such as related content) which I haven't been able to remove yet. Ideally, there should be a switch in the API. If this worked, the following snippet of code would retrieve the whole text without a second round of HTTP requests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# article_body = r.json()['response']['results'][0]['fields']['body']\n",
    "# article_body\n",
    "# new_soup = BeautifulSoup(article_body, 'html.parser')\n",
    "# ps2 = new_soup.find_all('p')\n",
    "# par_list = [p.text for p in ps2]\n",
    "# final2 = \" \".join(par_list)\n",
    "# final2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grab article urls and store them"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can repeat this process to grab as many articles as needed. We will search all articles containing the word \"Hong Kong\" in the body, from Jan 1 2019. This query returns thousands of hits, over many pages. It is convenient to increase the `page-size` of the server response to the maximum value (200) and to use a slightly different syntax for the HTTP request, so it's easier to iterate over the parameter `page`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's grab the first 2 pages (400 articles) of each category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_section_results(base_url, params, section, n_pages=3):\n",
    "    \n",
    "    section_results = []\n",
    "    current_page = 1\n",
    "    total_pages = n_pages\n",
    "    \n",
    "    while current_page <= total_pages:\n",
    "        params.update({'page': current_page, 'section': section})\n",
    "        try:\n",
    "            r = requests.get(base_url, params)\n",
    "            r.raise_for_status() \n",
    "        except requests.exceptions.RequestException as err:\n",
    "            raise SystemExit(err)\n",
    "            \n",
    "        data = r.json()\n",
    "        section_results.extend(data['response']['results'])\n",
    "        current_page += 1\n",
    "        \n",
    "    print(f\"Grabbed {len(section_results)} results.\")\n",
    "    return section_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def results_to_html(results, section, to_file=True):\n",
    "    \n",
    "    # grab urls, write to file\n",
    "    urls = [result['webUrl'] for result in results]\n",
    "    \n",
    "    if to_file:\n",
    "        with open(f\"urls_{section}.txt\", 'w') as f:\n",
    "            for url in urls:\n",
    "                f.write(f\"{url}\\n\")\n",
    "        print(f\"Written urls to urls_{section}.txt.\")\n",
    "\n",
    "    # retrieve HTML from urls\n",
    "    html_files = {}\n",
    "    while len(html_files) < len(urls): \n",
    "        \n",
    "        for i, url in enumerate(urls):\n",
    "            if i not in html_files:\n",
    "                try:\n",
    "                    file = requests.get(url)\n",
    "                    file.raise_for_status()\n",
    "                    html_files[i] = file\n",
    "                except requests.exceptions.RequestException as err:\n",
    "                    print(f\"At file {i}: {err}\")\n",
    "                    time.sleep(10)\n",
    "    \n",
    "    # tests\n",
    "    codes = set(file.status_code for file in html_files.values())\n",
    "    \n",
    "    if len(html_files) == len(urls):\n",
    "        print(f\"Retrieved html responses for all {len(urls)} urls. Status codes remaining: {codes}.\")\n",
    "        return html_files\n",
    "    else:\n",
    "        sys.exit(f\"Error: only got {len(html_files)} articles for {len(urls)} urls. Codes: {codes}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def html_to_text(html_files, section):\n",
    "    \n",
    "    all_texts = []\n",
    "    \n",
    "    for file_id, file in html_files.items():\n",
    "        soup = BeautifulSoup(file.content, 'html.parser')\n",
    "        body = soup.find_all('div', class_='article-body-commercial-selector')\n",
    "        \n",
    "        # items with no body are (few) liveblogs, can ignore\n",
    "        if len(body) == 1:\n",
    "            if section in ['film', 'culture']:\n",
    "                ps = body[0].find_all('p', class_='dcr-1m34hpq')\n",
    "            else:\n",
    "                ps = body[0].find_all('p', class_='dcr-s23rjr')\n",
    "            par_list = [p.text for p in ps]\n",
    "            text = \" \".join(par_list)\n",
    "            \n",
    "            # discard items not encompassed by above tags\n",
    "            if not (text is ''):\n",
    "                all_texts.append(text)\n",
    "            else:\n",
    "                # keep a record of \"bad\" urls\n",
    "                with open('bad_p.txt', 'a') as f:\n",
    "                    f.write(f\"Section {section}: {file.url}\\n\")\n",
    "    \n",
    "    print(f\"All done! Correctly parsed documents: {len(all_texts)}, discarded: {len(html_files)-len(all_texts)}.\")\n",
    "    return all_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    \n",
    "    API_ENDPOINT = \"http://content.guardianapis.com/search\"\n",
    "    sections = ['environment', 'business', 'film', 'culture', 'education']\n",
    "    my_params = {\n",
    "        'api-key': apikey,\n",
    "        'order-by': 'relevance', \n",
    "        'from-date': \"2020-1-1\",\n",
    "        'page-size': 200,\n",
    "    }\n",
    "    \n",
    "    for i, section in enumerate(sections):\n",
    "        \n",
    "        print(f\"[{i+1}/{len(sections)}] Requesting '{section}' articles from api...\")\n",
    "        results = get_section_results(API_ENDPOINT, my_params, section, n_pages=3)\n",
    "        \n",
    "        print(f\"Requesting html contents...\")\n",
    "        html_files = results_to_html(results, section, to_file=True)\n",
    "        \n",
    "        print(f\"Parsing and cleaning contents...\")\n",
    "        texts = html_to_text(html_files, section)\n",
    "        \n",
    "        print(f\"Saving dataframe...\")\n",
    "        df = pd.DataFrame({'Content': texts})\n",
    "        df.to_csv(f\"{section}_news.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/5] Requesting 'environment' articles from api...\n",
      "Grabbed 600 results.\n",
      "Requesting html contents...\n",
      "Written urls to urls_environment.txt.\n",
      "At file 127: 429 Client Error: Too Many Requests for url: https://www.theguardian.com/environment/2020/sep/21/coalitions-gas-plan-would-help-fewer-than-1-of-manufacturing-workers-report-finds\n",
      "At file 180: 429 Client Error: Too Many Requests for url: https://www.theguardian.com/environment/2020/nov/16/us-and-uk-yet-to-show-support-for-global-treaty-to-tackle-plastic-pollution\n",
      "At file 229: 429 Client Error: Too Many Requests for url: https://www.theguardian.com/environment/2020/oct/28/trump-permit-logging-alaska-tongass-national-forest\n",
      "At file 305: 429 Client Error: Too Many Requests for url: https://www.theguardian.com/environment/2020/jun/18/worst-outbreak-ever-nearly-a-million-pigs-culled-in-nigeria-due-to-swine-fever\n",
      "At file 368: 429 Client Error: Too Many Requests for url: https://www.theguardian.com/environment/2020/apr/29/sweet-city-the-costa-rica-suburb-that-gave-citizenship-to-bees-plants-and-trees-aoe\n",
      "At file 428: 429 Client Error: Too Many Requests for url: https://www.theguardian.com/environment/2020/jul/17/bearded-vulture-makes-rare-uk-visit-in-peak-district-national-park\n",
      "At file 459: 429 Client Error: Too Many Requests for url: https://www.theguardian.com/environment/2020/feb/29/country-diary-forward-into-spring-back-to-pre-raphaelite-days\n",
      "At file 527: 429 Client Error: Too Many Requests for url: https://www.theguardian.com/environment/2020/jan/20/animals-farmed-investigates-the-huge-global-trade-in-live-animals\n",
      "At file 558: 429 Client Error: Too Many Requests for url: https://www.theguardian.com/environment/2020/apr/15/like-a-spiral-ufo-worlds-longest-animal-discovered-in-australian-waters\n",
      "Retrieved html responses for all 600 urls. Status codes remaining: {200}.\n",
      "Parsing and cleaning contents...\n",
      "All done! Correctly parsed documents: 585, discarded: 15.\n",
      "Saving dataframe...\n",
      "[2/5] Requesting 'business' articles from api...\n",
      "Grabbed 600 results.\n",
      "Requesting html contents...\n",
      "Written urls to urls_business.txt.\n",
      "At file 332: 429 Client Error: Too Many Requests for url: https://www.theguardian.com/business/2020/nov/05/dont-become-a-pilot-as-there-are-no-jobs-just-huge-debts-says-union-balpa-covid-europe-unemployed\n",
      "At file 387: 429 Client Error: Too Many Requests for url: https://www.theguardian.com/business/2020/dec/09/small-business-coronavirus-pandemic-gene-marks\n",
      "At file 573: 429 Client Error: Too Many Requests for url: https://www.theguardian.com/business/2020/jun/28/lockdown-or-not-a-second-wave-of-covid-19-will-badly-damage-the-uk-economy\n",
      "Retrieved html responses for all 600 urls. Status codes remaining: {200}.\n",
      "Parsing and cleaning contents...\n",
      "All done! Correctly parsed documents: 475, discarded: 125.\n",
      "Saving dataframe...\n",
      "[3/5] Requesting 'film' articles from api...\n",
      "Grabbed 600 results.\n",
      "Requesting html contents...\n",
      "Written urls to urls_film.txt.\n",
      "At file 177: 429 Client Error: Too Many Requests for url: https://www.theguardian.com/film/2020/oct/05/belarus-filmmakers-capture-personal-stories-from-a-country-in-turmoil\n",
      "At file 223: 429 Client Error: Too Many Requests for url: https://www.theguardian.com/film/2020/nov/18/patrick-review-belgain-nudist-camp-drama\n",
      "At file 324: 429 Client Error: Too Many Requests for url: https://www.theguardian.com/film/2020/jun/30/john-lewis-congressman-us-good-trouble\n",
      "At file 375: 429 Client Error: Too Many Requests for url: https://www.theguardian.com/film/2020/jun/03/delroy-lindo-george-floyd-protests-da-5-bloods-spike-lee-british-racism-america\n",
      "At file 404: 429 Client Error: Too Many Requests for url: https://www.theguardian.com/film/2020/apr/20/beastie-boys-story-review-spike-jonze-apple-tv\n",
      "At file 442: 429 Client Error: Too Many Requests for url: https://www.theguardian.com/film/2020/may/08/ive-never-seen-8-1-2-federico-fellini\n",
      "At file 482: 429 Client Error: Too Many Requests for url: https://www.theguardian.com/film/2020/feb/28/wendy-benh-zeitlin-what-went-wrong\n",
      "At file 585: 429 Client Error: Too Many Requests for url: https://www.theguardian.com/film/2020/mar/01/downhill-review-will-ferrell-julia-louis-dreyfus-avalanche-drama\n",
      "Retrieved html responses for all 600 urls. Status codes remaining: {200}.\n",
      "Parsing and cleaning contents...\n",
      "All done! Correctly parsed documents: 599, discarded: 1.\n",
      "Saving dataframe...\n",
      "[4/5] Requesting 'culture' articles from api...\n",
      "Grabbed 600 results.\n",
      "Requesting html contents...\n",
      "Written urls to urls_culture.txt.\n",
      "At file 141: 429 Client Error: Too Many Requests for url: https://www.theguardian.com/culture/2020/may/27/i-watched-nothing-but-louis-theroux-films-for-a-whole-weird-weekend-and-heres-what-i-learned\n",
      "At file 164: 429 Client Error: Too Many Requests for url: https://www.theguardian.com/culture/2020/jun/29/alison-lester-beloved-author-and-illustrator-is-here-to-answer-your-childs-questions\n",
      "At file 286: 429 Client Error: Too Many Requests for url: https://www.theguardian.com/culture/2020/jan/27/jo-brand-face-no-further-action-battery-acid-joke\n",
      "At file 326: 429 Client Error: Too Many Requests for url: https://www.theguardian.com/culture/2020/jun/18/seth-meyers-coronavirus-trump-pandemic-late-night-tv\n",
      "At file 403: 429 Client Error: Too Many Requests for url: https://www.theguardian.com/culture/2021/aug/17/lily-cole-derided-on-social-media-burqa-selfies\n",
      "At file 473: 429 Client Error: Too Many Requests for url: https://www.theguardian.com/tv-and-radio/2021/apr/05/actor-thandiwe-newton-reclaims-original-spelling-of-her-name\n",
      "At file 525: 429 Client Error: Too Many Requests for url: https://www.theguardian.com/culture/2021/jun/19/on-my-radar-anne-enright-cultural-highlights\n",
      "At file 570: 429 Client Error: Too Many Requests for url: https://www.theguardian.com/culture/2021/aug/26/art-architecture-autumn-2021-hokusai-himid-salgado\n",
      "Retrieved html responses for all 600 urls. Status codes remaining: {200}.\n",
      "Parsing and cleaning contents...\n",
      "All done! Correctly parsed documents: 570, discarded: 30.\n",
      "Saving dataframe...\n",
      "[5/5] Requesting 'education' articles from api...\n",
      "Grabbed 600 results.\n",
      "Requesting html contents...\n",
      "Written urls to urls_education.txt.\n",
      "At file 160: 429 Client Error: Too Many Requests for url: https://www.theguardian.com/education/2020/nov/26/david-cassar-obituary\n",
      "At file 205: 429 Client Error: Too Many Requests for url: https://www.theguardian.com/education/2020/jun/29/now-more-than-ever-we-need-arts-graduates\n",
      "At file 266: 429 Client Error: Too Many Requests for url: https://www.theguardian.com/education/2020/apr/24/italy-home-schooling-coronavirus-lockdown-what-weve-learned\n",
      "At file 369: 429 Client Error: Too Many Requests for url: https://www.theguardian.com/education/2020/feb/19/ofsted-chief-amanda-spielman-schools-curriculum-funding-cuts\n",
      "At file 469: 429 Client Error: Too Many Requests for url: https://www.theguardian.com/education/2021/apr/01/teaching-assistants-unsung-heroes-of-pandemic-study-shows\n",
      "At file 523: 429 Client Error: Too Many Requests for url: https://www.theguardian.com/education/2021/may/19/cecil-rhodes-statue-at-oxford-college-should-go-says-independent-report\n",
      "Retrieved html responses for all 600 urls. Status codes remaining: {200}.\n",
      "Parsing and cleaning contents...\n",
      "All done! Correctly parsed documents: 524, discarded: 76.\n",
      "Saving dataframe...\n"
     ]
    }
   ],
   "source": [
    "!rm bad_p.txt\n",
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scrape article's body from url"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have all urls, let's retrieve all HTML. Since sometimes the server becomes overloaded and throws a 429 Error Code, we wait a bit before resuming our spamming. We store successful responses in a dictionary, so that it is simple to check which threw an error and still need to be retrieved."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can re-load the file to check everything is in order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "585\n",
      "475\n",
      "599\n",
      "570\n",
      "524\n"
     ]
    }
   ],
   "source": [
    "sections = ['environment', 'business', 'film', 'culture', 'education']\n",
    "\n",
    "for section in sections:\n",
    "    tmp = pd.read_csv(f\"{section}_news.csv\")\n",
    "    print(len(tmp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Content    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we are done. Another way to save text data would be to save each article to a separate .txt file, but for now this will suffice."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
